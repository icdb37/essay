{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd26230",
   "metadata": {},
   "source": [
    "# 线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ce740",
   "metadata": {},
   "source": [
    "## 标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2eff7ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T01:03:28.966248Z",
     "start_time": "2025-01-01T01:03:28.962263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.]), tensor([3.]), tensor([0.3333]), tensor([1.]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 只有一个元素的向量\n",
    "x = torch.tensor([1.0])\n",
    "y = torch.tensor([3.0])\n",
    "x+y,x*y,x/y,x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16c30c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T01:04:07.186212Z",
     "start_time": "2025-01-01T01:04:07.183596Z"
    }
   },
   "source": [
    "## 向量\n",
    "\n",
    "向量点积：对应元素相乘再相加；  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eaf8594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T01:24:21.139053Z",
     "start_time": "2025-01-01T01:24:21.135529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5., 4., 3., 2., 1.]), 5, torch.Size([5]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = torch.tensor([5.0,4.0,3.0,2.0,1.0])\n",
    "y = torch.arange(5,dtype=torch.float32)\n",
    "x,len(x),x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "071dc5cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T01:25:12.884482Z",
     "start_time": "2025-01-01T01:25:12.878911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(20.), tensor(20.))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 向量点积\n",
    "torch.dot(x,y),torch.sum(x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf3617",
   "metadata": {},
   "source": [
    "## 矩阵\n",
    "\n",
    "对称转置等于本身；\n",
    "矩阵运算：\n",
    "* 矩阵相乘：两个矩阵阶数一直，对应元素相乘；$C = A * B -> c_{i,j} = a_{i,j} * b_{i,j} $\n",
    "* 矩阵数乘：矩阵每个元素乘以数值；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34790f69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T01:10:52.849626Z",
     "start_time": "2025-01-01T01:10:52.845748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11]]),\n",
       " tensor([[ 0,  3,  6,  9],\n",
       "         [ 1,  4,  7, 10],\n",
       "         [ 2,  5,  8, 11]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(12).reshape(4,3)  # 二维矩阵\n",
    "B = torch.arange(24).reshape(2,3,4) # 三维矩阵\n",
    "A,A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "620b3144",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T01:12:31.562495Z",
     "start_time": "2025-01-01T01:12:31.558145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]]),\n",
       " False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = B.clone()   # 重新分配内存\n",
    "C,id(C) == id(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7554bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T01:22:58.398088Z",
     "start_time": "2025-01-01T01:22:58.393590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0,   1,   4],\n",
       "         [  9,  16,  25],\n",
       "         [ 36,  49,  64],\n",
       "         [ 81, 100, 121]]),\n",
       " tensor(66),\n",
       " tensor([18, 22, 26]),\n",
       " tensor([[18, 22, 26]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = A * A      # 矩阵乘法，对应元素相乘；\n",
    "asum = A.sum() # 所有元素相加\n",
    "asum0 = A.sum(axis=0) # 按照第一维度求和，a0 = a0j相加\n",
    "asumx = A.sum(axis=0, keepdims=True) # 求和之后保留阶数\n",
    "B,asum,asum0,asumx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b3a36",
   "metadata": {},
   "source": [
    "# 微积分\n",
    "\n",
    "## 标量导数\n",
    "$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx}f(x) = Df(x) = D_{x}f(x) $  其中$\\frac{d}{dx}$和$D$是微分运算符，表示微分操作。\n",
    "* $DC$ = 0 ($C$ 是一个常数）、\n",
    "* $Dx^{n} = nx^{n-1}$ （$n$ 是任意实数）\n",
    "* $De^{x} = e^{x}$\n",
    "* $Dln(x) = \\frac{1}{x}$\n",
    "\n",
    "运算法则：\n",
    "* 常数相乘： $\\frac{d}{dx}\\left[ Cf(x) \\right] = C\\frac{d}{dx}f(x) $\n",
    "* 加法法则：$\\frac{d}{dx}\\left[f(x) + g(x)\\right] = \\frac{d}{dx}f(x) + \\frac{d}{dx}g(x)$\n",
    "* 乘法法则：$\\frac{d}{dx}\\left[f(x)g(x)\\right] = f(x)\\frac{d}{dx}\\left[g(x)\\right] + g(x)\\frac{d}{dx}\\left[f(x)\\right]$\n",
    "* 除法法则：$\\frac{d}{dx}\\left[\\frac{f(x)}{g(x)}\\right] = \\frac{g(x)\\frac{d}{dx}\\left[f(x)\\right] - f(x)\\frac{d}{dx}\\left[g(x)\\right]}{\\left[g(x)\\right]^{2}}$\n",
    "\n",
    "### 偏导数\n",
    "设$y = f(x_{1},x_{2}, ... , x_{n})$是一个具有$n$个变量的函数。$y$关于第$i$个参数$x_{i}$的偏导数为：\n",
    "$$\\frac{\\partial y}{\\partial x_{i}} = \\underset{h->0}{lim}\\frac{f(x_{1},x_{2},...,x_{i}+h,...,x_{n}) - f(x_{1},...,x_{i},...,x_{n})}{h}$$\n",
    "计算时将除了$x_{i}$之外的变量视为常数；\n",
    "\n",
    "### 梯度\n",
    "多元函数偏导数组成的向量，即该函数的梯度向量。设函数$f:\\mathbb{R}^{n} -> \\mathbb{R}$的输入是一个n维向量 $\\mathbf{x} = [x_{1},x_{2},...,x_{n}]^\\top$，并且输入是一个标量。函数$f(\\mathbf{x})$相对于$\\mathbf{x}$的梯度是一个包含$n$个偏导数的向量：\n",
    "$$\\nabla _{x}f(\\mathbf{x}) = \\left[\\frac{\\partial f(\\mathbf x)}{\\partial x_{1}}, \\frac{\\partial f(\\mathbf x)}{\\partial x_{2}},...,\\frac{\\partial f(\\mathbf x)}{\\partial x_{n}}\\right]^\\top$$\n",
    "\n",
    "假设$\\mathbf{x}$n$维向量：  \n",
    "\n",
    "* 对于所有$\\mathbf{A} \\in \\mathbf{R}^{m \\times n}$，都有$\\nabla _{x}\\mathbf{Ax} = \\mathbf{A}^\\top$\n",
    "* 对于所有$\\mathbf{A} \\in \\mathbf{R}^{n \\times m}$，都有$\\nabla _{x}\\mathbf{x}^\\top\\mathbf{A} = \\mathbf{A}$\n",
    "* 对于所有$\\mathbf{A} \\in \\mathbf{R}^{n \\times n}$，都有$\\nabla _{x}\\mathbf{x}^\\top\\mathbf{Ax} = \\left(\\mathbf{A} + \\mathbf{A}^\\top \\right)\\mathbf{x}$\n",
    "* $\\nabla _{x} \\lVert x \\rVert^{2} = \\nabla _{x} \\mathbf{x^\\top x} = 2\\mathbf{x} $\n",
    "\n",
    "### 链式法则\n",
    "\n",
    "## 向量导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa602f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘图\n",
    "import matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d6887df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T10:21:20.627229Z",
     "start_time": "2025-01-01T10:21:20.623621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.10000, numerical_lim= 2.30000\n",
      "h=0.01000, numerical_lim= 2.03000\n",
      "h=0.00100, numerical_lim= 2.00300\n",
      "h=0.00010, numerical_lim= 2.00030\n",
      "h=0.00001, numerical_lim= 2.00003\n"
     ]
    }
   ],
   "source": [
    "# 函数定义\n",
    "def f(x):\n",
    "    return 3 * x **2 - 4 * x\n",
    "# 函数导数\n",
    "def numerical_lim(f,x,h):\n",
    "    return (f(x+h) - f(x)) / h\n",
    "# 求x=1时的导数，无限接近于2\n",
    "h = 0.1\n",
    "for i in range(5):\n",
    "    print(f'h={h:.5f}, numerical_lim={numerical_lim(f,1,h): .5f}')\n",
    "    h*= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56f24338",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T09:56:55.590823Z",
     "start_time": "2025-01-01T09:56:53.442777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算y关于x的梯度计算\n",
    "x = torch.arange(4.0, requires_grad=True)\n",
    "x.requires_grad_(True)\n",
    "x.grad  # 获取计算梯度结果\n",
    "y = 2 * torch.dot(x,x)\n",
    "y.backward()\n",
    "x.grad ,y\n",
    "\n",
    "x.grad.zero_() # 清除之前梯度结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be29636",
   "metadata": {},
   "source": [
    "### 自动微分\n",
    "\n",
    "深度学习框架通过自动计算导数，即自动微分来加快求导。自动微分使系统能够随后反向传播梯度。 这里，反向传播意味着跟踪整个计算图，填充关于每个参数的偏导数。\n",
    "\n",
    "$y = 2\\mathbf{x}^\\top\\mathbf{x}$ 关于向量$\\mathbf{x}$求导\n",
    "\n",
    "当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵。 对于高阶和高维的y和x，求导的结果可以是一个高阶张量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7de92f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)\n",
    "# x.torch.arange(4,requires_grad=True)\n",
    "y = 2 * torch.dot(x,x)\n",
    "y.backward() # 反向传播求导\n",
    "x.grad == 4 * x\n",
    "\n",
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f55f3",
   "metadata": {},
   "source": [
    "#### 分离计算\n",
    "有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。\n",
    "\n",
    "这里可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 换句话说，梯度不会向后流经u到x。 因此，下面的反向传播函数计算 $z=u*x$ 关于x的偏导数，同时将u作为常数处理， 而不是 $z=x*x*x$ 关于x的偏导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba62cdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.backward(torch.ones(len(x)))\n",
    "x.grad,y,x\n",
    "\n",
    "# 分离计算\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "z.backward(torch.ones(len(x)))\n",
    "x.grad,z,x,u  # 注意导数并不是 3x^2 而是u的值即x^2\n",
    "\n",
    "x.grad.zero_()\n",
    "y.sum().backward() # 等价于 y.backward(torch.ones(len(x)))\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdd2e82",
   "metadata": {},
   "source": [
    "#### 函数梯度\n",
    "\n",
    "使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d5f9545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(102400.), tensor(-1.2623, requires_grad=True))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "a.grad,a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7374fd0",
   "metadata": {},
   "source": [
    "# 概率\n",
    "\n",
    "\n",
    "**计算均值**：首先计算张量中所有元素的平均值（mean），对于一个多维张量，这通常意味着将所有维度上的元素加总后除以元素总数；\n",
    "\n",
    "**计算偏差平方和**：对于张量中的每一个元素，计算该元素与均值之间的差，然后将这个差值平方，并对所有的这些平方值求和；\n",
    "\n",
    "**方差**：将偏差平方和除以元素总数（样本标准差元素个数需要减一）；\n",
    "\n",
    "**标准方差**：将**方差**开根号；\n",
    "\n",
    "概率（probability）可以被认为是将集合映射到真实值的函数。在给定的样本空间$S$中，事件$A$的概率，表示为$P\\left(A\\right)$，满足以下属性：\n",
    "* 对于任意事件$A$，其概率非负，即$P\\left(A\\right) \\ge 0$\n",
    "* 整个样本空间的概率为1，即$P(S) \\eq 1$\n",
    "* 对于互斥事件（对于所有$i \\ne j$都有$A_{i} \\cap B_{j} = 0$）的任意序列$A_{1},A_{2},\\dot$发生的概率为它们各自发生的概率之和，即$P\\left(\\bigcup_{i=1}^{\\infty}A_{i}\\right) = \\sum_{i=1}^{\\infty}P\\left(A_{i}\\right)$\n",
    "\n",
    "离散（discrete）随机变量（如骰子的每一面） 和连续（continuous）随机变量（如人的体重和身高）之间存在微妙的区别。\n",
    "\n",
    "## 随机变量\n",
    "\n",
    "* **联合概率**：给定任意值$a$和$b$，$P(A = a, B = b) \\le P(A = a)$\n",
    "* **条件概率**：$0 \\le \\frac{P(A=a, B=b)}{P(A=a)} \\le 1$ 即$P(B = b| A = a)$，以$A=a$为前提下$B=b$发生的概率\n",
    "* **贝叶斯定理**：$P(A,B) = P(B|A)P(A) = P(A|B)P(B)$ 当 $P(B) \\gt 0$ 时有 $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "* **边际化**：$P(B) = \\underset{A}{\\sum}P(A,B)$\n",
    "* **独立性**：$P(A,B) = P(A)P(B)$\n",
    "\n",
    "\n",
    "## 期望和方差\n",
    "\n",
    " 一个随机变量$X$的期望表示为：$E\\left[X\\right] = \\underset{x}{\\sum}xP(X = x)$\n",
    " 当函数$f(x)$的输入是分布$P$中抽取的随机变量时，$f(x)$期望为：$E_{x~P} = \\underset{x}{\\sum}f(x)P(x)$\n",
    " \n",
    " 衡量随机变量$X$与期望值的偏置，方差：$Var[X] = E[(X - E[X])^2] = E[X^2] = E[X]^2$\n",
    " \n",
    " 方差的平方根被称为标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "969bc4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]),\n",
       " tensor([0.1430, 0.1670, 0.1730, 0.1880, 0.1540, 0.1750]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions import multinomial\n",
    "from d2l import torch as d2l\n",
    "\n",
    "fair_probs = torch.ones([6]) / 6  # 概率分布\n",
    "counts = multinomial.Multinomial(1000, fair_probs).sample() # 生成随机样本\n",
    "fair_probs, counts / 1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
